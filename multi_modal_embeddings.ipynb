{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUmFoQIeCD0uTS1k5MgyLN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dev-nileshpawar/python-aiml/blob/main/multi_modal_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tabula\n",
        "!pip install boto3\n",
        "!pip install faiss-cpu\n",
        "!pip install pymupdf\n",
        "!pip install langchain_text_splitters\n",
        "!pip install langchain\n",
        "!pip install tabula-py\n"
      ],
      "metadata": {
        "id": "bS494pStzhy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7xuzdYxozaS7"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import tabula\n",
        "import faiss\n",
        "import json\n",
        "import base64\n",
        "import pymupdf\n",
        "import requests\n",
        "import os\n",
        "import logging\n",
        "import numpy as np\n",
        "# import warning\n",
        "from tqdm import tqdm\n",
        "from botocore.exceptions import ClientError\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from IPython import display\n",
        "\n",
        "# logger = logging.getLogger(__name__)\n",
        "# logger.setLevel(logging.DEBUG)\n",
        "\n",
        "# warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://arxiv.org/pdf/1706.03760\"\n",
        "response = requests.get(url)\n",
        "filename = \"attention_paper.pdf\"\n",
        "filepath = os.path.join(\"data\", filename)\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "with open(filepath, \"wb\") as f:\n",
        "  if response.status_code==200:\n",
        "    f.write(response.content)\n",
        "    print(f\"File downloaded successfully: {filepath}\")\n",
        "  else:\n",
        "    print(f\"Failed to download file. Status code: {response.status_code}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODwdvSs_zfvM",
        "outputId": "faab0d43-c933-49b0-8e3a-4078f5b0bbd7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully: data/attention_paper.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S45dENXKzsdU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data extractio"
      ],
      "metadata": {
        "id": "Rt4DRS9F0bcx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CMS4MFzm0cKM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 align=\"center\">Multi modal RAG with Amazon Bedrock, Amazon Nova and LangChain</h1>\n"
      ],
      "metadata": {
        "id": "cA81zMSZ0gn6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DAn_b-i00g_W"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 align=\"center\"><b>Customize a Foundation Model</b></h1>\n",
        "### 1. Instruction-based Fine-Tuning\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  +------------------------------+\n",
        "  | Task-Specific Labeled Data   |\n",
        "  +------------------------------+\n",
        "                |\n",
        "                v   Fine-Tuning\n",
        "                |\n",
        "                v\n",
        "  +------------------------------+\n",
        "  |            LLM               | <---------------------- User/System Prompt\n",
        "  +------------------------------+\n",
        "```\n",
        "\n",
        "\n",
        "### 2. Domain adoption\n",
        "```\n",
        "  +--------------------------------+\n",
        "  | Domain specific unlebeled data |\n",
        "  +--------------------------------+\n",
        "                   |\n",
        "                   v  Continious pre training\n",
        "                   |\n",
        "                   v\n",
        "  +--------------------------------+\n",
        "  |            LLM                 |<----------------user/system prompts\n",
        "  +--------------------------------+\n",
        "```\n",
        "\n",
        "\n",
        "### 3. Informative Retrieval\n",
        "1. convert knowledge data (Audio, video, Image, Text) into embeddings and store vectors into vector DB\n",
        "2. whenever user sends a query then we search into vector DB for relevant info (data chunk)\n",
        "3. we call LLM by passing relavant info chunk and user query and extract final answer.\n",
        "\n",
        "```\n",
        "    +-----------------------------------+\n",
        "    |  Domain specific unlabeled data   |\n",
        "    +-----------------------------------+\n",
        "                     |   Embeddings\n",
        "                     V   \n",
        "                     |   prompt\n",
        "                     v\n",
        "                     |   Prompt with context\n",
        "                     v\n",
        "       +---------------------------+\n",
        "       |           LLM             |\n",
        "       +---------------------------+\n",
        "```\n"
      ],
      "metadata": {
        "id": "ZLt0aKmV0jbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wrapper function to extract the data from file\n",
        "\n",
        "def create_directories(base_path):\n",
        "  directories = [\"images\", \"text\", \"tables\", \"page_images\"]\n",
        "  for dir in directories:\n",
        "    if not os.path.exists(os.path.join(base_path, dir)):\n",
        "      os.makedirs(os.path.join(base_path, dir))\n",
        "\n",
        "def process_tables(doc, page_num, base_dir, items):\n",
        "  try:\n",
        "    tables = tabula.read_pdf(filepath, pages = page_num+1, multiple_tables=True)\n",
        "    # print(tables)\n",
        "    for table_id, table in enumerate(tables):\n",
        "      table_text = \"\\n\".join([\" | \".join(map(str, row)) for row in table.values])\n",
        "      table_file_name = f\"{base_dir}/tables/{os.path.basename(filepath)}_table_{page_num}_{table_id}.txt\"\n",
        "\n",
        "      with open(table_file_name, \"w\") as f:\n",
        "        f.write(table_text)\n",
        "        items.append({\"page\": page_num, \"type\":\"table\", \"text\": table_text, \"path\": table_file_name})\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    return\n",
        "def process_text_chunks(text, text_splitter, page_num, base_dir, items):\n",
        "  text_chunks = text_splitter.split_text(text)\n",
        "  for chunk_id, chunk in enumerate(text_chunks):\n",
        "    text_file_name = f\"{base_dir}/text/{os.path.basename(filepath)}_text_{page_num}_{chunk_id}.txt\"\n",
        "    with open(text_file_name, \"w\") as f:\n",
        "      f.write(chunk)\n",
        "      items.append({\"page\": page_num, \"type\": \"text\", \"text\": chunk, \"path\": text_file_name})\n",
        "  return\n",
        "def process_images(page, page_num, base_dir, items):\n",
        "  image_list = page.get_images()\n",
        "  for image_id, image in enumerate(image_list):\n",
        "    xref = image[0]\n",
        "    pix = pymupdf.Pixmap(doc, xref)\n",
        "    image_file_name = f\"{base_dir}/images/{os.path.basename(filepath)}_image_{page_num}_{image_id}_{xref}.png\"\n",
        "    pix.save(image_file_name)\n",
        "    with open(image_file_name, \"rb\") as f:\n",
        "      image_bytes = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "    items.append({\"page\": page_num, \"type\": \"image\", \"path\":image_id, \"image\": image_bytes})\n",
        "  return\n",
        "def process_page_images(page, page_num, base_dir, items):\n",
        "  return"
      ],
      "metadata": {
        "id": "bUPlGZ1O0j5b"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = pymupdf.open(filepath)\n",
        "num_pages = len(doc)\n",
        "base_dir = \"data\"\n",
        "\n",
        "create_directories(base_dir)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, length_function=len)\n",
        "items = []\n",
        "\n",
        "# process each page of the pdf\n",
        "for page_num in tqdm(range(num_pages)):\n",
        "  page = doc.load_page(page_num)\n",
        "  text = page.get_text(\"text\")\n",
        "  process_tables(doc, page_num, base_dir, items)\n",
        "  process_text_chunks(text, text_splitter, page_num, base_dir, items)\n",
        "  process_images(page, page_num, base_dir, items)\n",
        "  process_page_images(page, page_num, base_dir, items)\n"
      ],
      "metadata": {
        "id": "oxSmJZK60wNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_multimodal_embedding(prompt=None, image=None, output_embedding_length=384):\n",
        "  if not prompt and not image:\n",
        "    raise ValueError(\"prompt or image must be provided\")\n",
        "\n",
        "  model_id = \"amazon.titan-embed-image-v1\"\n",
        "  body = {\"embeddingConfig\": {\"outputEmbeddingLength\": output_embedding_length}}\n",
        "\n",
        "  if prompt:\n",
        "    body[\"inputText\"] = prompt\n",
        "  if image:\n",
        "    body[\"inputImage\"] = image\n",
        "\n",
        "  try:\n",
        "    response = client.invoke_model(\n",
        "        body=json.dumps(body),\n",
        "        modelId=model_id,\n",
        "        accept=\"application/json\",\n",
        "        contentType=\"application/json\"\n",
        "    )\n",
        "    response_body = json.loads(response.get(\"body\").read())\n",
        "    embedding = response_body[\"embedding\"]\n",
        "    return embedding\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "5RlVwTSz0yem"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "PRO_MODEL_ID = \"amazon.nova-pro-v1:0\"\n",
        "LITE_MODEL_ID = \"amazon.nova-lite-v1:0\"\n",
        "MACRO_MODEL_ID = \"amazon.nova-macro-v1:0\"\n",
        "\n",
        "YOUR_ACCESS_KEY = userdata.get('YOUR_ACCESS_KEY')\n",
        "YOUR_SECRET_KEY = userdata.get(\"YOUR_SECRET_KEY\")\n",
        "print(\"--------\", YOUR_ACCESS_KEY)\n",
        "print(\"--------\", YOUR_SECRET_KEY)\n",
        "client = boto3.client(\n",
        "    service_name=\"bedrock-runtime\",\n",
        "    region_name=\"us-east-1\",\n",
        "    aws_access_key_id=YOUR_ACCESS_KEY,\n",
        "    aws_secret_access_key=YOUR_SECRET_KEY,\n",
        ")"
      ],
      "metadata": {
        "id": "N5O0H3Bv05Wn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ce57541-777c-4c39-fc24-e832b1fdac65"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_vector_dimension = 384\n",
        "\n",
        "item_counts = {\n",
        "  \"text\": sum(1 for item in items if item[\"type\"] == \"text\"),\n",
        "  \"table\": sum(1 for item in items if item[\"type\"] == \"table\"),\n",
        "  \"image\": sum(1 for item in items if item[\"type\"] == \"image\"),\n",
        "  \"page\": sum(1 for item in items if item[\"type\"] == \"page\")\n",
        "}\n",
        "\n",
        "counters = dict.fromkeys(item_counts.keys(), 0)\n",
        "bar_format=\"{l_bar}/{bar}| {n_fmt}/{total_fmt} [{elapsed} < {remaining}, {rate_fmt}{postfix}]\"\n",
        "\n",
        "print(\"---\", item_counts)\n",
        "with tqdm(\n",
        "    total=len(items),\n",
        "    desc=\"Generating embeddings\",\n",
        "    bar_format=bar_format\n",
        ")as pbar:\n",
        "  for item in items:\n",
        "    item_type = item[\"type\"]\n",
        "    counters[item_type]+=1\n",
        "    if item_type in [\"text\", \"table\"]:\n",
        "      item[\"embedding\"] = generate_multimodal_embedding(prompt=item[\"text\"], output_embedding_length=embedding_vector_dimension)\n",
        "    elif item_type in [\"image\"]:\n",
        "      item[\"embedding\"] = generate_multimodal_embedding(image=item[\"image\"], output_embedding_length=embedding_vector_dimension)\n",
        "\n",
        "  pbar.set_postfix_str(f\"Text: {counters['text']}/{item_counts['text']}, Table: {counters['table']}/{item_counts['table']}, Image: {counters['image']}/{item_counts['image']}\")\n",
        "\n",
        "  pbar.update(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "San3cezq02NI",
        "outputId": "9229ce26-1346-48dd-d388-cacd5e22494d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- {'text': 52, 'table': 1, 'image': 26, 'page': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating embeddings:   1%|/▏         | 1/79 [00:18 < 23:28, 18.06s/it, Text: 52/52, Table: 1/1, Image: 26/26]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "items[0][\"text\"]"
      ],
      "metadata": {
        "id": "B6Se7mm52r9X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "80ef8e4e-9f6d-42bc-a96e-4705eda7ab72"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Operational quasiprobabilities for continuous variables\\nJeongwoo Jae,1 Junghee Ryu,2, ∗and Jinhyoung Lee1, †\\n1Department of Physics, Hanyang University, Seoul, 133-791, Republic of Korea\\n2Centre for Quantum Technologies, National University of Singapore, 3 science Drive 2, 117543 Singapore, Singapore\\nWe generalize the operational quasiprobability involving sequential measurements proposed by\\nRyu et al. [Phys. Rev. A 88, 052123] to a continuous-variable system. The quasiprobabilities in\\nquantum optics are incommensurate, i.e., they represent a given physical observation in diﬀerent\\nmathematical forms from their classical counterparts, making it diﬃcult to operationally interpret\\ntheir negative values. Our operational quasiprobability is commensurate, enabling one to compare\\nquantum and classical statistics on the same footing. We show that the operational quasiprobability\\ncan be negative against the hypothesis of macrorealism for various states of light.\\nQuadrature'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_embeddings = all_embeddings = np.array(\n",
        "    [item[\"embedding\"] for item in items if item.get(\"embedding\") is not None],\n",
        "    dtype=\"float32\"\n",
        ")\n",
        "\n",
        "index = faiss.IndexFlatL2(embedding_vector_dimension)\n",
        "index.reset()\n",
        "\n",
        "index.add(np.array(all_embeddings, dtype=np.float32))"
      ],
      "metadata": {
        "id": "sY399Y9t5mCI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain_aws"
      ],
      "metadata": {
        "id": "Ef7v8N6O89bf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_aws import ChatBedrock\n",
        "\n",
        "def invoke_nova_multimodal(prompt, matched_items):\n",
        "  system_message = [{\n",
        "      \"text\": \"\"\"You are an helpful assistant for question asnwering,\n",
        "        The text context is relavant information retrieved.\n",
        "        The provided image(s) are relavant information retrieved.\n",
        "        Answer if answer is available in provided context otherwise return \\\"answer not found\\\" reply\n",
        "      \"\"\"\n",
        "  }]\n",
        "\n",
        "  message_content = []\n",
        "  for item in matched_items:\n",
        "    if item[\"type\"] == \"text\" or item[\"type\"] ==\"table\":\n",
        "      message_content.append({\"text\": item[\"text\"]})\n",
        "    else:\n",
        "      message_content.append({\"image\": item[\"image\"]})\n",
        "\n",
        "    inf_params = {\n",
        "        \"max_new_tokens\" : 300,\n",
        "        \"top_p\":0.9,\n",
        "        \"top_k\":30,\n",
        "    }\n",
        "\n",
        "    message_list = [{\n",
        "      \"role\":\"user\", \"content\" : message_content\n",
        "    }]\n",
        "\n",
        "    native_request = {\n",
        "        \"message\": message_list,\n",
        "        \"system\": system_message,\n",
        "        \"inferenceConfig\": inf_params\n",
        "    }\n",
        "\n",
        "    model_id = \"amazon.nova-pro-v1:0\"\n",
        "    client = ChatBedrock(\n",
        "        model_id=model_id,\n",
        "        aws_access_key_id=YOUR_ACCESS_KEY,\n",
        "        aws_secret_access_key=YOUR_SECRET_KEY,\n",
        "        region_name=\"us-east-1\",\n",
        "        )\n",
        "\n",
        "    response = client.invoke(json.dumps(native_request))\n",
        "\n",
        "    model_response = response.content\n",
        "    return model_response\n",
        "\n"
      ],
      "metadata": {
        "id": "DvFjPlEi7M5y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"how long were the base and bigg models trained\"\n",
        "query_embedding = generate_multimodal_embedding(prompt=query, output_embedding_length=embedding_vector_dimension)\n",
        "\n",
        "\n",
        "result = index.search(np.array(query_embedding, dtype=np.float32).reshape(1, -1), k=5)\n",
        "\n",
        "print('====',result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTa52boq9BJT",
        "outputId": "4921322e-944c-4bac-baa6-5f2025ed488a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== (array([[1.3791456, 1.3848724, 1.3874055, 1.411334 , 1.411334 ]],\n",
            "      dtype=float32), array([[19,  5, 40, 13, 14]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "D, I = result\n",
        "\n",
        "matched_items = [\n",
        "    {k: v for k, v in items[idx].items() if k != \"embedding\"}\n",
        "    for idx in I[0]\n",
        "]\n",
        "print(matched_items)\n",
        "response = invoke_nova_multimodal(query, matched_items)\n",
        "\n",
        "display.Markdown(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "WLRvl1JDEZcu",
        "outputId": "7ca58f84-f204-40d8-e018-ee4c0f0e7cf8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'page': 2, 'type': 'text', 'text': 'consider a classical model assuming realism and nonin-\\nvasive measurability. Classical physics has been consid-\\nered as the realistic theory which assumes predetermined\\nphysical quantities before the actual measurements. This\\nimplies the existence of an underlying joint probability\\ndistribution for the outcomes of all possible measure-\\nments.\\nIn a temporal scenario, Leggett and Garg examined\\nnoninvasive measurability at the macroscopic level. One\\ncan measure a physical quantity of a macroscopic object\\nwithout disturbing it. This hypothesis together with re-\\nalism, called macrorealism (MR), leads the Leggett-Garg\\ninequality involving temporal correlations [11]. It shows\\nthat quantum prediction is incompatible with the clas-\\nsical one. More precisely, MR is deﬁned by the follow-\\ning three hypotheses [29, 30]: “Macrorealism per se. A\\nmacroscopic object which has available to it two or more\\nmacroscopically distinct states is at any given time in a', 'path': 'data/text/attention_paper.pdf_text_2_2.txt'}, {'page': 0, 'type': 'text', 'text': 'the positivity of the quasiprobabilities.\\nThe condition\\nof no signaling in time [19–21] is considered a speciﬁc\\nnoninvasive measurability.\\nTo test macrorealism, the Leggett-Garg inequality has\\nbeen employed, consisting of temporal correlations be-\\ntween bounded variables [11, 22]. In this case, the ob-\\nservables need to be binned to dichotomic outcomes or\\nbounded in the ﬁnite range, say, the interval [−1, 1] in the\\nmacrorealism tests of CV systems [23–25]. In Ref. [20],\\nunbounded observables for CV were considered to test\\nthe condition of no signaling in time; however the ex-\\nperimental scheme is not known yet. We propose an ex-\\nperimental scheme to realize the sequential CV measure-\\nments of quasiprobabilities.\\nWe also discuss the relation of the negativity of op-\\nerational quasiprobability to the nonclassicality of light,\\ntypically witnessed with the Glauber-Sudarshan P func-\\ntion [3]. In a conventional view, coherent states and their', 'path': 'data/text/attention_paper.pdf_text_0_5.txt'}, {'page': 4, 'type': 'text', 'text': '5\\nB.\\nResults\\nAs we pointed out before, the negativity of OQCV is\\ndetermined by the overlap between the given state and\\nthe measurement bases. In Fig. 3, we plot the OQCV\\nas a function of ⃗β by ﬁxing ⃗α (in general, the OQCV\\nlives in four-dimensional space; thus we ﬁx one measure-\\nment basis for plots). We consider the vacuum, number\\nstate |2⟩, and squeezed vacuum states of ¯nsq = 5. The\\nblue regions of each plot denote the negative values of\\nthe OQCV. We also observe the behavior of moving the\\npositive (red) regions of the OQCV for number state |2⟩\\nas changing the M1 basis from ⃗α = (0, 0) to (1, 1), [see\\nFig. 3(d)].\\nLet us now examine how the overlap contributes to the\\nnegativity of the OQCV for a coherent state |w⟩. The\\nOQCV reads\\nW|w⟩(⃗α, ⃗β) = 1\\nπ2 |⟨w|α⟩|2 |⟨α|β⟩|2\\n+ 1\\n2π e−1\\n2 |α|2 \\x12 1\\nπ |⟨w|β⟩|2 −1\\n2π |⟨w|β⟩|\\n\\x13\\n,\\n(11)\\nwhere the second term in the bracket, the marginal prob-\\nability P(⃗β|M1, M2) in Eq. (10), was obtained marginally', 'path': 'data/text/attention_paper.pdf_text_4_0.txt'}, {'page': 1, 'type': 'image', 'path': 0, 'image': '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'}, {'page': 1, 'type': 'image', 'path': 1, 'image': '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'}]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the provided context, the classical model under consideration assumes realism and non-invasive measurability. Classical physics is viewed as a realistic theory where physical quantities are predetermined before measurements, implying an underlying joint probability distribution for all possible measurement outcomes.\n\nIn a temporal scenario, Leggett and Garg investigated non-invasive measurability at the macroscopic level, proposing that one can measure a physical quantity of a macroscopic object without disturbing it. This, combined with realism, forms the concept of macrorealism (MR). MR leads to the Leggett-Garg inequality, which involves temporal correlations and demonstrates that quantum predictions are incompatible with classical ones.\n\nMacrorealism is defined by three hypotheses:\n1. **Macrorealism per se**: A macroscopic object that can exist in two or more macroscopically distinct states is, at any given time, in a definite state.\n\nNo further answer can be derived from the provided context."
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9y3e0l2ZFww1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
