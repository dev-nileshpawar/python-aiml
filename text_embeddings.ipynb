{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMG3lIMIqz3JHp6JCdWemiB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dev-nileshpawar/python-aiml/blob/main/text_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oxswW30V2B4P"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = re.split(r'([,.?_!\"()\\']|--|\\s)', \"Hello, world. This is a test.\")\n",
        "result = [item.strip() for item in res if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "id": "mO1y5NmV2GPq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bd5820f-6e91-4237-8f75-c2cc2b15b1c8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'This', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res1 = re.split(r'\\s', \"Name name is nilesh\")"
      ],
      "metadata": {
        "id": "bxdiNUCKCvez"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6vfnE_qDBUC",
        "outputId": "4833483d-ab67-411c-b0eb-55d9690e4ee7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Name', 'name', 'is', 'nilesh']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4pn4twEDDIZ",
        "outputId": "01ee9669-b798-4a62-f108-6265acc2956d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " ',',\n",
              " '',\n",
              " ' ',\n",
              " 'world',\n",
              " '.',\n",
              " '',\n",
              " ' ',\n",
              " 'This',\n",
              " ' ',\n",
              " 'is',\n",
              " ' ',\n",
              " 'a',\n",
              " ' ',\n",
              " 'test',\n",
              " '.',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./sample_data/the-verdict.txt\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()"
      ],
      "metadata": {
        "id": "a5y0RN1dDUym"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYvUvW02FcFI",
        "outputId": "69c22c3b-c83f-4b2d-fcea-4fa895c07bbf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "\n",
        "print(preprocessed[:30])\n",
        "print(len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2cckHA7Fy_D",
        "outputId": "3cb7bda5-affb-49c1-a95f-d36de584d84b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n",
            "4649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))"
      ],
      "metadata": {
        "id": "QkDSyqR_HKhz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer, token in enumerate(all_words)}"
      ],
      "metadata": {
        "id": "rFwm8zMQHY3v"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if(i==20):\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POdC8LqHIc_5",
        "outputId": "d6c316f9-5460-452d-bf76-f50e9557a220"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        # Mapping from string → integer token ID\n",
        "        self.str_to_int = vocab\n",
        "\n",
        "        # Reverse mapping from integer → string token\n",
        "        self.int_to_str = {}\n",
        "        for token_str, token_id in vocab.items():\n",
        "            self.int_to_str[token_id] = token_str\n",
        "\n",
        "    def encode(self, text):\n",
        "        # Split text into words + punctuation\n",
        "        parts = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        # Clean parts (remove empty and whitespace-only items)\n",
        "        preprocessed = []\n",
        "        for item in parts:\n",
        "            stripped = item.strip()\n",
        "            if stripped:\n",
        "                preprocessed.append(stripped)\n",
        "\n",
        "        # Convert tokens to IDs\n",
        "        ids = []\n",
        "        for token in preprocessed:\n",
        "            ids.append(self.str_to_int[token])\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        # Convert IDs back to tokens\n",
        "        tokens = []\n",
        "        for token_id in ids:\n",
        "            tokens.append(self.int_to_str[token_id])\n",
        "\n",
        "        # Join tokens with spaces\n",
        "        text = \" \".join(tokens)\n",
        "\n",
        "        # Remove unwanted space before punctuation\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "\n",
        "        return text\n"
      ],
      "metadata": {
        "id": "Yg8BUbuGYPZB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_encode = SimpleTokenizerV1(vocab=vocab)\n",
        "encoded_text = token_encode.encode(\"My is\")\n",
        "print(token_encode.decode(encoded_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h19I0G7aZa1",
        "outputId": "e7586a61-8760-4a7d-ada4-db90d38f7c5a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "\n",
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer, token in enumerate(all_tokens)}"
      ],
      "metadata": {
        "id": "LKg6vIQVchcj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(vocab.items())[-10:]):\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph_c0JJJd0zl",
        "outputId": "66c833ef-a9d2-4981-fc35-41702e3254be"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('year', 1151)\n",
            "('years', 1152)\n",
            "('yellow', 1153)\n",
            "('yet', 1154)\n",
            "('you', 1155)\n",
            "('younger', 1156)\n",
            "('your', 1157)\n",
            "('yourself', 1158)\n",
            "('<|endoftext|>', 1159)\n",
            "('<|unk|>', 1160)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Encoding**"
      ],
      "metadata": {
        "id": "-rtKKg_qi1Db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        # Mapping from string → integer token ID\n",
        "        self.str_to_int = vocab\n",
        "\n",
        "        # Reverse mapping from integer → string token\n",
        "        self.int_to_str = {}\n",
        "        for token_str, token_id in vocab.items():\n",
        "            self.int_to_str[token_id] = token_str\n",
        "\n",
        "    def encode(self, text):\n",
        "        # Split text into words + punctuation\n",
        "        parts = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        # Clean parts (remove empty and whitespace-only items)\n",
        "        preprocessed = []\n",
        "        for item in parts:\n",
        "            stripped = item.strip()\n",
        "            if stripped:\n",
        "                preprocessed.append(stripped)\n",
        "\n",
        "        # Convert tokens to IDs\n",
        "        ids = []\n",
        "        for token in preprocessed:\n",
        "          if token in self.str_to_int:\n",
        "            ids.append(self.str_to_int[token])\n",
        "          else:\n",
        "            ids.append(\"<|unk|>\")\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        # Convert IDs back to tokens\n",
        "        tokens = []\n",
        "        for token_id in ids:\n",
        "          if token_id in self.int_to_str:\n",
        "            tokens.append(self.int_to_str[token_id])\n",
        "          else:\n",
        "            tokens.append(\"<|unk|>\")\n",
        "\n",
        "        # Join tokens with spaces\n",
        "        text = \" \".join(tokens)\n",
        "\n",
        "        # Remove unwanted space before punctuation\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "\n",
        "        return text\n"
      ],
      "metadata": {
        "id": "jhyQbu89eYDL"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizerv2 = SimpleTokenizerV2(vocab=vocab)\n",
        "encoded_text = tokenizerv2.encode(\"My name is\")\n",
        "print(tokenizerv2.decode(encoded_text))\n",
        "# print(encoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh9K4KZ-e65H",
        "outputId": "0afb104d-981d-46f5-d4ba-a3a770f5c973"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My <|unk|> is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "\n",
        "text = \"<|endoftext|>\".join((text1, text2))"
      ],
      "metadata": {
        "id": "5FAERDtKgZOP"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5D0DtWcgpNm",
        "outputId": "7ac004e5-7b60-4f61-b208-432e807c59af"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea?<|endoftext|>In the sunlit terraces of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_encoded = tokenizerv2.encode(text)\n",
        "print(tokenizerv2.decode(text_encoded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKT8FnUkgr3s",
        "outputId": "772f04cd-eac1-4225-fccc-07f8e138b0d6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|unk|>, do you like tea? <|unk|> the sunlit terraces of the <|unk|>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Byte Pair Encoding**"
      ],
      "metadata": {
        "id": "7VpVERCMiIMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no code for Byte Pair encoding"
      ],
      "metadata": {
        "id": "Y4oIWr8_iNSX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Use TikToken Library to create tokens**"
      ],
      "metadata": {
        "id": "QnYQKbW_1Sd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "nX9NSeYN1Xrd"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=tiktoken.get_encoding(\"gpt2\");\n",
        "tokenizer.n_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HpYlXlW1b1U",
        "outputId": "15551030-6d41-410d-c713-5c6ea006035b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tiktoken.list_encoding_names()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9YJfkDg1lM-",
        "outputId": "9823fd70-7018-4942-96b0-5e8c78b2a14c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gpt2',\n",
              " 'r50k_base',\n",
              " 'p50k_base',\n",
              " 'p50k_edit',\n",
              " 'cl100k_base',\n",
              " 'o200k_base',\n",
              " 'o200k_harmony']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "    \"of someunknownword.\"\n",
        ")\n",
        "\n",
        "integer = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(integer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFTkE_Ra1o_3",
        "outputId": "82154808-051c-4cd2-b462-4fdcc77c8d30"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 4775, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(integer)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tECkY633ArZ",
        "outputId": "220cae04-b0c0-4c06-c3d3-cfd9c91afafd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownword.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "str=\"LLM learn to predict one word at a time.\"\n",
        "\n",
        "for i, item in enumerate(str.split(' ')):\n",
        "  print(\" \".join(str.split(\" \")[0:i+1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUGm9d0eMaQB",
        "outputId": "1ce447ec-f73a-4232-a210-8675258b079c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM\n",
            "LLM learn\n",
            "LLM learn to\n",
            "LLM learn to predict\n",
            "LLM learn to predict one\n",
            "LLM learn to predict one word\n",
            "LLM learn to predict one word at\n",
            "LLM learn to predict one word at a\n",
            "LLM learn to predict one word at a time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "tYrKH9bfM87a"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class MyDataset (Dataset):\n",
        "  def __init__(self, text, tokenizer, max_length, stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "    for i in range(0, len(token_ids)-max_length, stride):\n",
        "      input_chunk = token_ids[i:i+max_length]\n",
        "      target_chunk = token_ids[i+1:i+1+max_length]\n",
        "\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "V-BYsj51j_VK"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "  dataset = MyDataset(txt, tokenizer, max_length, stride)\n",
        "\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "ZmOWLl9Nkde6"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./sample_data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()"
      ],
      "metadata": {
        "id": "kD0xuFOAnBHB"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "    raw_text,\n",
        "    batch_size=1,\n",
        "    max_length=4,\n",
        "    stride=1,\n",
        "    shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "e1EAMM1xnJxd"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "id": "hNuVVPVTojLg",
        "outputId": "5f046254-329e-423b-d89a-5f1fcb2b2a98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[8104,  866, 1973,  262]]), tensor([[  866,  1973,   262, 37918]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "id": "8GdVMn6lorgl",
        "outputId": "5da81787-47c5-4fa0-c83b-5b772cd2b7d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 938, 4842, 1650,  353]]), tensor([[4842, 1650,  353,  438]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=4, stride=4, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs,targets = next(data_iter)\n",
        "print(\"input: \", inputs)\n",
        "print(\"targets: \", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dB9Pp7RboxEa",
        "outputId": "68766cde-9a57-454d-8356-4f918e409ab2"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input:  tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "targets:  tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor([2,3,5,1])"
      ],
      "metadata": {
        "id": "j3v7DAS3p85I"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=6\n",
        "output_dim=3\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "cvKo42Juy3hH"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "id": "ZoWCTWMiy5A7",
        "outputId": "fd3bb44d-7685-4f1b-c58c-1e72bef3fdd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(torch.tensor([3])))"
      ],
      "metadata": {
        "id": "CsifXWXTzHu9",
        "outputId": "7f141c5d-344a-4a9b-b9f7-a4831ce21ea9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_token_ids = torch.tensor([2,3,5,1])\n",
        "print(embedding_layer(input_token_ids))"
      ],
      "metadata": {
        "id": "HcK_eDTW8iic",
        "outputId": "cdae4b67-2746-473f-8c7d-721cd3c61603",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Encoding word positions**"
      ],
      "metadata": {
        "id": "5dLiUowL9PJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "m3EN5I699qn8"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "\n",
        "print(\"tokenids\", inputs)\n",
        "print(\"input shape\", inputs.shape)"
      ],
      "metadata": {
        "id": "dyBGK2bbAhRv",
        "outputId": "ae641929-554c-4c0d-da8f-20cb086c2d7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenids tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "input shape torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "id": "3saMLNwABHfA",
        "outputId": "94f9c288-3c40-4f22-9fba-1cf039969aab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n"
      ],
      "metadata": {
        "id": "XOkbGFf2B_Ed"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embeddings=pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "id": "XGNMiX3NFjrZ",
        "outputId": "11bb989a-4dd7-4ddc-ebf8-cc0c4a4738ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings"
      ],
      "metadata": {
        "id": "8wb662MAEvG5"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings"
      ],
      "metadata": {
        "id": "IPcm9epEEwSl",
        "outputId": "d0644f08-8528-4aa1-ab9a-036a25833d8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 2.2288,  0.5619,  0.8286,  ..., -0.6272, -0.2987,  0.8900],\n",
              "         [ 2.0903, -0.4664, -0.0593,  ...,  0.9115, -1.0493, -1.6473],\n",
              "         [-0.7158, -0.8304,  1.2494,  ...,  2.3952,  1.8773,  0.8051],\n",
              "         [ 0.2703,  0.4029,  3.0514,  ...,  0.3595, -1.4548,  0.8310]],\n",
              "\n",
              "        [[ 3.2835,  1.1749, -1.4150,  ..., -0.3281,  2.4332,  0.6924],\n",
              "         [-0.2199, -0.9114, -0.1750,  ...,  1.5337, -0.1998,  0.1462],\n",
              "         [ 1.5197, -1.4240,  0.4391,  ...,  1.0494, -1.4318,  2.3057],\n",
              "         [ 0.2893,  0.8346, -0.1884,  ...,  1.9602,  0.8709,  0.8796]],\n",
              "\n",
              "        [[ 0.9662,  0.0952, -0.4640,  ..., -1.0320,  1.6290,  1.7771],\n",
              "         [ 2.4468, -0.2154,  1.4984,  ...,  1.8766,  0.5595, -0.1423],\n",
              "         [-0.3856, -2.5393,  1.1556,  ...,  3.6157,  1.3267,  0.4944],\n",
              "         [-0.2487, -0.5275,  2.0009,  ...,  0.2930,  0.5977,  1.3300]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 0.1219,  0.3991, -3.2740,  ..., -1.1921,  2.6637,  2.6728],\n",
              "         [ 1.2438, -1.6436, -1.1101,  ..., -0.7464, -0.9816,  0.5118],\n",
              "         [ 0.1439, -0.2428,  0.7786,  ...,  0.8001, -1.5986,  2.4871],\n",
              "         [-0.3077, -0.6329,  0.0536,  ...,  1.5188, -0.2060,  0.4254]],\n",
              "\n",
              "        [[ 1.6095,  0.0535,  1.0871,  ...,  0.1512,  1.0996,  2.5603],\n",
              "         [ 2.1284, -2.4306,  0.6478,  ...,  0.5593, -1.6896,  1.4126],\n",
              "         [-1.4224, -0.0750,  1.9386,  ...,  3.3712, -2.4016, -0.3237],\n",
              "         [-0.4752, -1.2234, -0.0847,  ...,  0.7834, -1.0744,  0.3429]],\n",
              "\n",
              "        [[ 0.7802,  0.1387,  0.7277,  ...,  1.7101, -0.3304, -0.1471],\n",
              "         [ 1.5791, -1.3749, -0.8234,  ..., -0.5420, -0.3528, -0.5756],\n",
              "         [ 0.1382,  0.1226,  2.6528,  ...,  2.9576, -0.2933,  0.5577],\n",
              "         [ 0.4520, -0.5711,  1.2128,  ...,  1.3198, -2.5226,  0.4127]]],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TY00hlwuOso1"
      },
      "execution_count": 58,
      "outputs": []
    }
  ]
}